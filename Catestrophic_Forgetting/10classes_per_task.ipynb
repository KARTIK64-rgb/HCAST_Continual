{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-12T04:20:07.123771Z",
     "iopub.status.busy": "2025-06-12T04:20:07.123087Z",
     "iopub.status.idle": "2025-06-12T04:20:26.190761Z",
     "shell.execute_reply": "2025-06-12T04:20:26.189905Z",
     "shell.execute_reply.started": "2025-06-12T04:20:07.123743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:20:26.192596Z",
     "iopub.status.busy": "2025-06-12T04:20:26.192187Z",
     "iopub.status.idle": "2025-06-12T04:20:26.314971Z",
     "shell.execute_reply": "2025-06-12T04:20:26.314337Z",
     "shell.execute_reply.started": "2025-06-12T04:20:26.192571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#  Base path\n",
    "base_path = \"/kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data\"\n",
    "\n",
    "#  Load mapping files\n",
    "def load_mapping(file_path, label_type):\n",
    "    mapping = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            if len(parts) == 2:\n",
    "                mapping.append((parts[0], parts[1]))\n",
    "    return pd.DataFrame(mapping, columns=[\"image\", label_type])\n",
    "\n",
    "variant_df = load_mapping(f\"{base_path}/images_variant_trainval.txt\", \"variant\")\n",
    "family_df = load_mapping(f\"{base_path}/images_family_trainval.txt\", \"family\")\n",
    "maker_df = load_mapping(f\"{base_path}/images_manufacturer_trainval.txt\", \"maker\")\n",
    "\n",
    "#  Merge labels on 'image'\n",
    "df = variant_df.merge(family_df, on=\"image\").merge(maker_df, on=\"image\")\n",
    "\n",
    "#  Group by variant to get canonical family & maker\n",
    "hierarchy = df.groupby(\"variant\")[[\"family\", \"maker\"]].agg(lambda x: x.mode().iloc[0]).reset_index()\n",
    "\n",
    "#  Create ID maps\n",
    "variant_to_id = {v: i for i, v in enumerate(hierarchy[\"variant\"].sort_values().unique())}\n",
    "family_to_id = {f: i for i, f in enumerate(hierarchy[\"family\"].sort_values().unique())}\n",
    "maker_to_id = {m: i for i, m in enumerate(hierarchy[\"maker\"].sort_values().unique())}\n",
    "\n",
    "#  Map IDs\n",
    "hierarchy[\"variant_id\"] = hierarchy[\"variant\"].map(variant_to_id)\n",
    "hierarchy[\"family_id\"] = hierarchy[\"family\"].map(family_to_id)\n",
    "hierarchy[\"maker_id\"] = hierarchy[\"maker\"].map(maker_to_id)\n",
    "\n",
    "#  Optional: Create lookup dicts for later use\n",
    "variant_to_family = hierarchy.set_index(\"variant_id\")[\"family_id\"].to_dict()\n",
    "variant_to_maker = hierarchy.set_index(\"variant_id\")[\"maker_id\"].to_dict()\n",
    "\n",
    "#  Save CSV\n",
    "output_path = \"/kaggle/working/hierarchy_full.csv\"\n",
    "hierarchy.to_csv(output_path, index=False)\n",
    "\n",
    "#  Confirm\n",
    "print(\" hierarchy_full.csv saved with columns:\", hierarchy.columns.tolist())\n",
    "print(f\" Classes: {len(variant_to_id)} variants, {len(family_to_id)} families, {len(maker_to_id)} makers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:20:26.316065Z",
     "iopub.status.busy": "2025-06-12T04:20:26.315797Z",
     "iopub.status.idle": "2025-06-12T04:20:26.491963Z",
     "shell.execute_reply": "2025-06-12T04:20:26.491206Z",
     "shell.execute_reply.started": "2025-06-12T04:20:26.316045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Step 1: Prepare variant to family/maker hierarchy and task splits\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Path to original FGVC Aircraft data\n",
    "base_path = \"/kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data\"\n",
    "\n",
    "# Load mapping from original trainval files\n",
    "def load_mapping(file_path, label_type):\n",
    "    mapping = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            if len(parts) == 2:\n",
    "                mapping.append((parts[0], parts[1]))\n",
    "    return pd.DataFrame(mapping, columns=[\"image\", label_type])\n",
    "\n",
    "variant_df = load_mapping(f\"{base_path}/images_variant_trainval.txt\", \"variant\")\n",
    "family_df = load_mapping(f\"{base_path}/images_family_trainval.txt\", \"family\")\n",
    "maker_df = load_mapping(f\"{base_path}/images_manufacturer_trainval.txt\", \"maker\")\n",
    "\n",
    "# Merge into a full image-level dataframe\n",
    "df = variant_df.merge(family_df, on=\"image\").merge(maker_df, on=\"image\")\n",
    "\n",
    "# Get the variant-level hierarchy (unique mapping assumed)\n",
    "hierarchy = df.groupby(\"variant\")[[\"family\", \"maker\"]].agg(lambda x: x.mode()[0]).reset_index()\n",
    "\n",
    "# Save the full hierarchy\n",
    "hierarchy_path = \"/kaggle/working/hierarchy_full.csv\"\n",
    "hierarchy.to_csv(hierarchy_path, index=False)\n",
    "\n",
    "#  Step 2: Split variants into 10 tasks randomly\n",
    "variants = hierarchy[\"variant\"].tolist()\n",
    "random.seed(42)\n",
    "random.shuffle(variants)\n",
    "\n",
    "num_tasks = 10\n",
    "variants_per_task = len(variants) // num_tasks\n",
    "\n",
    "# Build task split dictionary with full hierarchy per variant\n",
    "task_hierarchy = {}\n",
    "for i in range(num_tasks):\n",
    "    task_name = f\"task_{i+1}\"\n",
    "    task_variants = variants[i*variants_per_task:(i+1)*variants_per_task]\n",
    "    task_data = []\n",
    "    for var in task_variants:\n",
    "        fam = hierarchy[hierarchy[\"variant\"] == var].iloc[0][\"family\"]\n",
    "        mak = hierarchy[hierarchy[\"variant\"] == var].iloc[0][\"maker\"]\n",
    "        task_data.append({\"variant\": var, \"family\": fam, \"manufacturer\": mak})\n",
    "    task_hierarchy[task_name] = task_data\n",
    "\n",
    "# Save task split to JSON\n",
    "with open(\"/kaggle/working/task_split.json\", \"w\") as f:\n",
    "    json.dump(task_hierarchy, f, indent=4)\n",
    "\n",
    "print(\" Task split with hierarchy saved to task_split.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:20:26.493785Z",
     "iopub.status.busy": "2025-06-12T04:20:26.493536Z",
     "iopub.status.idle": "2025-06-12T04:20:30.736099Z",
     "shell.execute_reply": "2025-06-12T04:20:30.735291Z",
     "shell.execute_reply.started": "2025-06-12T04:20:26.493765Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.2)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->scikit-image) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->scikit-image) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->scikit-image) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24->scikit-image) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24->scikit-image) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:20:30.737629Z",
     "iopub.status.busy": "2025-06-12T04:20:30.737290Z",
     "iopub.status.idle": "2025-06-12T04:30:51.774950Z",
     "shell.execute_reply": "2025-06-12T04:30:51.774132Z",
     "shell.execute_reply.started": "2025-06-12T04:20:30.737593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.segmentation import slic\n",
    "from skimage.util import img_as_float\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  Paths\n",
    "base_path = \"/kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data\"\n",
    "img_dir = os.path.join(base_path, \"images\")\n",
    "seg_dir = \"/kaggle/working/segment_maps\"\n",
    "mapping_file = \"/kaggle/working/mapping.txt\"\n",
    "task_split_path = \"/kaggle/working/task_split.json\"\n",
    "variant_label_file = os.path.join(base_path, \"images_variant_trainval.txt\")\n",
    "\n",
    "#  Create output directory\n",
    "os.makedirs(seg_dir, exist_ok=True)\n",
    "mapping = []\n",
    "\n",
    "#  Step 1: Load task split\n",
    "with open(task_split_path, \"r\") as f:\n",
    "    task_split = json.load(f)\n",
    "\n",
    "#  Step 2: Get list of allowed variants from all tasks\n",
    "all_variants = set()\n",
    "for task_data in task_split.values():\n",
    "    for entry in task_data:\n",
    "        all_variants.add(entry[\"variant\"])\n",
    "\n",
    "#  Step 3: Safe parser for image â†’ variant mapping\n",
    "def load_variant_mapping(file_path):\n",
    "    records = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            if len(parts) == 2:\n",
    "                image, variant = parts\n",
    "                records.append((image, variant))\n",
    "    return pd.DataFrame(records, columns=[\"image\", \"variant\"])\n",
    "\n",
    "variant_df = load_variant_mapping(variant_label_file)\n",
    "variant_df = variant_df[variant_df[\"variant\"].isin(all_variants)].drop_duplicates().sort_values(\"image\")\n",
    "\n",
    "#  Step 4: Generate segment maps only for selected images\n",
    "print(f\" Generating superpixel maps for {len(variant_df)} images in task variant split...\")\n",
    "\n",
    "skipped = 0\n",
    "\n",
    "for _, row in tqdm(variant_df.iterrows(), total=len(variant_df)):\n",
    "    image_id = row[\"image\"]\n",
    "    img_path = os.path.join(img_dir, f\"{image_id}.jpg\")\n",
    "    \n",
    "    if not os.path.exists(img_path):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        resized = image.resize((224, 224))\n",
    "        image_np = img_as_float(np.array(resized))\n",
    "\n",
    "        segments = slic(image_np, n_segments=196, compactness=10, start_label=0)\n",
    "        flat = segments.flatten()\n",
    "        encoder = LabelEncoder()\n",
    "        encoded = encoder.fit_transform(flat)\n",
    "        normalized_segments = encoded.reshape(224, 224)\n",
    "\n",
    "        seg_tensor = torch.from_numpy(normalized_segments).long()\n",
    "        seg_save_path = os.path.join(seg_dir, image_id + \".pt\")\n",
    "        torch.save(seg_tensor, seg_save_path)\n",
    "\n",
    "        mapping.append(f\"{image_id} segment_maps/{image_id}.pt\")\n",
    "    except Exception as e:\n",
    "        print(f\" Skipping {image_id}: {e}\")\n",
    "        skipped += 1\n",
    "\n",
    "#  Save mapping file\n",
    "with open(mapping_file, \"w\") as f:\n",
    "    f.writelines([line + \"\\n\" for line in mapping])\n",
    "\n",
    "#  Summary\n",
    "print(f\"\\n Completed generation of superpixel maps.\")\n",
    "print(f\" mapping.txt saved at {mapping_file}\")\n",
    "print(f\"  Total segment maps created: {len(mapping)}\")\n",
    "print(f\" Skipped or failed images: {skipped}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:51.776243Z",
     "iopub.status.busy": "2025-06-12T04:30:51.775852Z",
     "iopub.status.idle": "2025-06-12T04:30:51.913310Z",
     "shell.execute_reply": "2025-06-12T04:30:51.912113Z",
     "shell.execute_reply.started": "2025-06-12T04:30:51.776222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p cast_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:51.914977Z",
     "iopub.status.busy": "2025-06-12T04:30:51.914669Z",
     "iopub.status.idle": "2025-06-12T04:30:51.923396Z",
     "shell.execute_reply": "2025-06-12T04:30:51.922364Z",
     "shell.execute_reply.started": "2025-06-12T04:30:51.914938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cast_models/aircraft_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cast_models/aircraft_dataset.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class AircraftHCASTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Aircraft HCAST Dataset for Hierarchical Classification.\n",
    "    Includes an optional flag to include segment maps or not.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, label_file, image_split_file, mapping_file,\n",
    "                 transform=None, use_segments=True, debug=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.debug = debug\n",
    "        self.use_segments = use_segments\n",
    "\n",
    "        #  Default image transform\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        #  Load hierarchy\n",
    "        self.hierarchy_df = pd.read_csv(label_file)\n",
    "        self.variant_map = {\n",
    "            row[\"variant\"].strip().lower(): (row[\"variant_id\"], row[\"family_id\"], row[\"maker_id\"])\n",
    "            for _, row in self.hierarchy_df.iterrows()\n",
    "        }\n",
    "        self.num_variants = self.hierarchy_df[\"variant_id\"].nunique()\n",
    "        self.num_families = self.hierarchy_df[\"family_id\"].nunique()\n",
    "        self.num_makers = self.hierarchy_df[\"maker_id\"].nunique()\n",
    "\n",
    "        #  Segment path mapping\n",
    "        if self.use_segments:\n",
    "            with open(mapping_file, \"r\") as f:\n",
    "                self.seg_dict = {\n",
    "                    line.strip().split()[0]: line.strip().split()[1]\n",
    "                    for line in f if line.strip()\n",
    "                }\n",
    "\n",
    "        #  Init sample storage\n",
    "        self.samples = []\n",
    "        self.variants = []\n",
    "        self.families = []\n",
    "        self.makers = []\n",
    "        self.variant_names = []\n",
    "\n",
    "        known_variants = set(self.variant_map.keys())\n",
    "\n",
    "        with open(image_split_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                try:\n",
    "                    base_name, variant = line.strip().split(\" \", 1)\n",
    "                except ValueError:\n",
    "                    if self.debug:\n",
    "                        print(f\" Malformed line: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                variant_norm = variant.strip().lower()\n",
    "                img_name = base_name + \".jpg\"\n",
    "\n",
    "                if variant_norm not in known_variants:\n",
    "                    if self.debug:\n",
    "                        print(f\" Unknown variant: {variant}\")\n",
    "                    continue\n",
    "\n",
    "                # Segment file check only if use_segments is True\n",
    "                seg_rel_path = None\n",
    "                if self.use_segments:\n",
    "                    if base_name not in self.seg_dict:\n",
    "                        if self.debug:\n",
    "                            print(f\" Missing segment map: {base_name}\")\n",
    "                        continue\n",
    "                    seg_rel_path = self.seg_dict[base_name]\n",
    "                    seg_abs_path = os.path.join(\"/kaggle/working\", seg_rel_path)\n",
    "                    if not os.path.exists(seg_abs_path):\n",
    "                        if self.debug:\n",
    "                            print(f\" Segment file missing: {seg_abs_path}\")\n",
    "                        continue\n",
    "\n",
    "                variant_id, family_id, maker_id = self.variant_map[variant_norm]\n",
    "                self.samples.append((img_name, variant_norm, seg_rel_path))\n",
    "                self.variants.append(variant_id)\n",
    "                self.families.append(family_id)\n",
    "                self.makers.append(maker_id)\n",
    "                self.variant_names.append(variant.strip())\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\" Loaded {len(self.samples)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, variant_norm, seg_rel_path = self.samples[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "\n",
    "        #  Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_tensor = self.transform(image)\n",
    "\n",
    "        #  Load segment map if enabled\n",
    "        segment_map = None\n",
    "        if self.use_segments:\n",
    "            seg_path = os.path.join(\"/kaggle/working\", seg_rel_path)\n",
    "            segment_map = torch.load(seg_path)\n",
    "\n",
    "        #  Hierarchical labels\n",
    "        variant_id, family_id, maker_id = self.variant_map[variant_norm]\n",
    "\n",
    "        return {\n",
    "            \"image\": image_tensor,\n",
    "            \"segments\": segment_map,\n",
    "            \"variant\": torch.tensor(variant_id, dtype=torch.long),\n",
    "            \"family\": torch.tensor(family_id, dtype=torch.long),\n",
    "            \"maker\": torch.tensor(maker_id, dtype=torch.long),\n",
    "            \"image_name\": img_name\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:51.924576Z",
     "iopub.status.busy": "2025-06-12T04:30:51.924335Z",
     "iopub.status.idle": "2025-06-12T04:30:51.945076Z",
     "shell.execute_reply": "2025-06-12T04:30:51.944289Z",
     "shell.execute_reply.started": "2025-06-12T04:30:51.924560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cast_models/cast.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cast_models/cast.py\n",
    "#  Imports\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import PatchEmbed, trunc_normal_\n",
    "\n",
    "from cast_models.utils import segment_mean_nd\n",
    "from cast_models.graph_pool import GraphPooling\n",
    "from cast_models.modules import Pooling, ConvStem\n",
    "\n",
    "_all_ = ['cast_small', 'cast_small_deep', 'cast_base', 'cast_base_deep']\n",
    "\n",
    "class CAST(VisionTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        nb_classes = kwargs.pop('nb_classes', None)\n",
    "        dropout_rate = kwargs.pop('dropout_rate', 0.1)\n",
    "        assert nb_classes is not None, \"You must provide nb_classes\"\n",
    "\n",
    "        depths = kwargs['depth']\n",
    "        num_clusters = kwargs.pop('num_clusters', [64, 32, 16, 8])\n",
    "        kwargs['depth'] = sum(depths)\n",
    "        kwargs['drop_rate'] = dropout_rate\n",
    "        kwargs['attn_drop_rate'] = dropout_rate\n",
    "        kwargs['drop_path_rate'] = dropout_rate\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        if hasattr(self, 'dist_token'):\n",
    "            del self.dist_token\n",
    "        if hasattr(self, 'head_dist'):\n",
    "            del self.head_dist\n",
    "\n",
    "        if len(nb_classes) == 3:\n",
    "            self.total_classes, self.num_family, self.num_manufacturer = nb_classes\n",
    "        elif len(nb_classes) == 2:\n",
    "            self.total_classes, self.num_family = nb_classes\n",
    "            self.num_manufacturer = 0\n",
    "        else:\n",
    "            self.total_classes = nb_classes[0]\n",
    "            self.num_family = 0\n",
    "            self.num_manufacturer = 0\n",
    "\n",
    "        #  Fixed classification heads (no dynamic expansion)\n",
    "        self.head = nn.Linear(self.embed_dim, self.total_classes)\n",
    "        self.family_head = nn.Linear(self.embed_dim, self.num_family) if self.num_family > 0 else nn.Identity()\n",
    "        self.manufacturer_head = nn.Linear(self.embed_dim, self.num_manufacturer) if self.num_manufacturer > 0 else nn.Identity()\n",
    "\n",
    "        patch_H, patch_W = self.patch_embed.grid_size\n",
    "        num_patches = patch_H * patch_W\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, self.embed_dim))\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "        cumsum = [0]\n",
    "        for d in depths:\n",
    "            cumsum.append(cumsum[-1] + d)\n",
    "        assert sum(depths) == len(self.blocks)\n",
    "\n",
    "        blocks, pools = [], []\n",
    "        for i, d in enumerate(depths):\n",
    "            blocks.append(self.blocks[cumsum[i]:cumsum[i + 1]])\n",
    "            pool = Pooling(pool_block=GraphPooling(\n",
    "                num_clusters=num_clusters[i],\n",
    "                d_model=self.embed_dim,\n",
    "                l2_normalize_for_fps=False\n",
    "            ))\n",
    "            if i == len(depths) - 1:\n",
    "                for m in (pool.pool_block.fc1, pool.pool_block.fc2, pool.pool_block.centroid_fc):\n",
    "                    for p in m.parameters():\n",
    "                        p.requires_grad = False\n",
    "            pools.append(pool)\n",
    "\n",
    "        self.blocks1, self.pool1 = blocks[0], pools[0]\n",
    "        self.blocks2, self.pool2 = blocks[1], pools[1]\n",
    "        self.blocks3, self.pool3 = blocks[2], pools[2]\n",
    "        self.blocks4, self.pool4 = blocks[3], pools[3]\n",
    "\n",
    "    def _block_operations(self, x, cls_token, pad_mask, nn_block, pool_block, norm_block):\n",
    "        cls_x = torch.cat([cls_token, x], dim=1)\n",
    "        cls_x = nn_block(cls_x).type_as(x)\n",
    "        cls_token, x = cls_x[:, :1], cls_x[:, 1:]\n",
    "\n",
    "        cls_token, logit, centroid, new_mask, inds = pool_block(cls_token, x, pad_mask)\n",
    "        out = norm_block(cls_x)[:, 0] if norm_block else cls_x[:, 0]\n",
    "        return x, cls_token, logit, centroid, new_mask, inds, out\n",
    "\n",
    "    def forward_features(self, x, y):\n",
    "        x = self.patch_embed(x)\n",
    "        N, H, W, C = x.shape\n",
    "        y = F.interpolate(y.unsqueeze(1).float(), (H, W), mode='nearest').squeeze(1).long()\n",
    "        ones = torch.ones((N, H, W, 1), device=x.device)\n",
    "        avg = segment_mean_nd(ones, y).squeeze(-1)\n",
    "        pad = avg <= 0.5\n",
    "        x = segment_mean_nd(x, y)\n",
    "        pos = segment_mean_nd(\n",
    "            self.pos_embed[:, 1:].view(1, H, W, C).expand(N, -1, -1, -1), y\n",
    "        )\n",
    "        x = self.pos_drop(x + pos)\n",
    "        cls = self.cls_token.expand(N, -1, -1) + self.pos_embed[:, :1]\n",
    "\n",
    "        x, cls1, _, cent1, pad1, _, _ = self._block_operations(x, cls, pad, self.blocks1, self.pool1, None)\n",
    "        x, cls2, _, cent2, pad2, _, out2 = self._block_operations(cent1, cls1, pad1, self.blocks2, self.pool2, None)\n",
    "        x, cls3, _, cent3, pad3, _, out3 = self._block_operations(cent2, cls2, pad2, self.blocks3, self.pool3, None)\n",
    "        _, _, _, _, _, _, out4 = self._block_operations(cent3, cls3, pad3, self.blocks4, self.pool4, self.norm)\n",
    "\n",
    "        return out2, out3, out4\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out2, out3, out4 = self.forward_features(x, y)\n",
    "        logits = self.head(out2)\n",
    "        fam_logits = self.family_head(out3)\n",
    "        man_logits = self.manufacturer_head(out4)\n",
    "        return logits, fam_logits, man_logits\n",
    "\n",
    "@register_model\n",
    "def cast_small(pretrained=False, **kwargs):\n",
    "    model = CAST(\n",
    "        patch_size=8, embed_dim=384, num_clusters=[64, 32, 16, 8],\n",
    "        depth=[3, 3, 3, 2], num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), embed_layer=ConvStem,\n",
    "        **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def cast_small_deep(pretrained=False, **kwargs):\n",
    "    model = CAST(\n",
    "        patch_size=8, embed_dim=384, num_clusters=[64, 32, 16, 8],\n",
    "        depth=[6, 3, 3, 3], num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), embed_layer=ConvStem,\n",
    "        **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def cast_base(pretrained=False, **kwargs):\n",
    "    model = CAST(\n",
    "        patch_size=8, embed_dim=768, num_clusters=[64, 32, 16, 8],\n",
    "        depth=[3, 3, 3, 2], num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), embed_layer=ConvStem,\n",
    "        **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def cast_base_deep(pretrained=False, **kwargs):\n",
    "    model = CAST(\n",
    "        patch_size=8, embed_dim=768, num_clusters=[64, 32, 16, 8],\n",
    "        depth=[6, 3, 3, 3], num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), embed_layer=ConvStem,\n",
    "        **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:51.946873Z",
     "iopub.status.busy": "2025-06-12T04:30:51.946232Z",
     "iopub.status.idle": "2025-06-12T04:30:51.964472Z",
     "shell.execute_reply": "2025-06-12T04:30:51.963767Z",
     "shell.execute_reply.started": "2025-06-12T04:30:51.946853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cast_models/graph_pool.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cast_models/graph_pool.py\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from cast_models.utils import farthest_point_sampling\n",
    "from cast_models.utils import segment_mean_nd\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim must be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, qkv_bias=False, drop=0., attn_drop=0., norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads, qkv_bias, attn_drop, drop)\n",
    "        self.drop_path = nn.Identity()\n",
    "        self.bias = nn.Parameter(torch.zeros(dim).normal_(0, 1e-2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm(x)))\n",
    "        x = x - x.mean(dim=1, keepdim=True) + self.bias.view(1, 1, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphPooling(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_clusters=4,\n",
    "        d_model=512,\n",
    "        dropout=0.1,\n",
    "        l2_normalize_for_fps=True,\n",
    "        num_heads=12,\n",
    "        qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._num_clusters = num_clusters\n",
    "        self._l2_normalize_for_fps = l2_normalize_for_fps\n",
    "\n",
    "        self.centroid_fc = Block(d_model, num_heads, qkv_bias, norm_layer=norm_layer)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model * 4),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "    def _fill_with_mean(self, src, mask):\n",
    "        bs, sl, cs = src.shape\n",
    "        if mask is not None:\n",
    "            mean_src = valid_mean(src, ~mask).unsqueeze(1).type_as(src)\n",
    "            fill_mask = mask.unsqueeze(2).expand(-1, -1, cs)\n",
    "            filled_src = torch.where(fill_mask, mean_src.expand(-1, sl, -1), src)\n",
    "        else:\n",
    "            mean_src = src.mean(dim=1, keepdim=True)\n",
    "            filled_src = src\n",
    "        return filled_src, mean_src\n",
    "\n",
    "    def forward(self, cls_token, src, mask=None):\n",
    "        bs, sl, cs = src.shape\n",
    "\n",
    "        if sl < self._num_clusters:\n",
    "            raise ValueError(f\"Too few tokens ({sl}) to sample {self._num_clusters} clusters.\")\n",
    "\n",
    "        filled_src, mean_src = self._fill_with_mean(src, mask)\n",
    "        padded_src = torch.cat([mean_src, filled_src], dim=1)\n",
    "\n",
    "        sampling_src = F.normalize(padded_src, dim=-1) if self._l2_normalize_for_fps else padded_src\n",
    "        sampled_inds = farthest_point_sampling(sampling_src, self._num_clusters).clamp(0, sl - 1)\n",
    "        unfold_inds = sampled_inds.unsqueeze(2).expand(-1, -1, cs)\n",
    "\n",
    "        node_features = self.centroid_fc(src)\n",
    "        centroid_feats = torch.gather(node_features, 1, unfold_inds)\n",
    "\n",
    "        norm_node = F.normalize(node_features, dim=-1)\n",
    "        norm_centroid = F.normalize(centroid_feats, dim=-1)\n",
    "        logits = torch.matmul(norm_node, norm_centroid.transpose(1, 2)) * 5\n",
    "        assignments = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        fc1_cls_src = self.fc1(torch.cat([cls_token, src], dim=1))\n",
    "        fc1_cls, fc1_src = fc1_cls_src[:, :1], fc1_cls_src[:, 1:]\n",
    "\n",
    "        normalizer = assignments.transpose(1, 2) @ torch.ones((bs, sl, 1), dtype=src.dtype, device=src.device)\n",
    "        centroid = assignments.transpose(1, 2) @ fc1_src\n",
    "        centroid = centroid / (normalizer + 1e-6)\n",
    "\n",
    "        fc2_out = self.fc2(torch.cat([fc1_cls, centroid], dim=1))\n",
    "        new_cls = fc2_out[:, :1, :] + cls_token\n",
    "        new_centroid = fc2_out[:, 1:, :] + torch.gather(src, 1, unfold_inds)\n",
    "\n",
    "        out = new_cls.squeeze(1)  # Final graph representation\n",
    "        new_mask = None  # Placeholder for hierarchy-aware masking\n",
    "\n",
    "        return new_cls, logits, new_centroid, new_mask, sampled_inds, out\n",
    "\n",
    "\n",
    "def valid_mean(x, mask):\n",
    "    mask = mask.type_as(x).unsqueeze(2)  # [B, N, 1]\n",
    "    sum_mask = torch.clamp(mask.sum(dim=1), min=1)\n",
    "    masked_x = x * mask\n",
    "    mean_x = masked_x.sum(dim=1) / sum_mask\n",
    "    return mean_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:51.967579Z",
     "iopub.status.busy": "2025-06-12T04:30:51.966825Z",
     "iopub.status.idle": "2025-06-12T04:30:51.986240Z",
     "shell.execute_reply": "2025-06-12T04:30:51.985624Z",
     "shell.execute_reply.started": "2025-06-12T04:30:51.967559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cast_models/modules.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cast_models/modules.py\n",
    "\"\"\"Define shared modules\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.layers.helpers import to_2tuple\n",
    "\n",
    "\n",
    "class Pooling(nn.Module):\n",
    "\n",
    "    def __init__(self, pool_block):\n",
    "        super(Pooling, self).__init__()\n",
    "        self.pool_block = pool_block\n",
    "\n",
    "    def forward(self, cls_token, x, padding_mask=None):\n",
    "        cls_token, pool_logit, centroid, new_mask, sampled_x_inds, out = self.pool_block(cls_token=cls_token, src=x, mask=padding_mask)\n",
    "\n",
    "        pool_padding_mask = torch.zeros(\n",
    "            (pool_logit.shape[0], pool_logit.shape[-1]),\n",
    "            dtype=torch.bool,\n",
    "            device=pool_logit.device)\n",
    "\n",
    "        return cls_token, pool_logit, centroid, pool_padding_mask, sampled_x_inds\n",
    "\n",
    "\n",
    "class ConvStem(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=8, in_chans=3, embed_dim=768, norm_layer=None, flatten=False, **kwargs):\n",
    "        super().__init__()\n",
    "        assert patch_size == 8, 'ConvStem only supports patch size of 8'\n",
    "        assert embed_dim % 8 == 0, 'Embed dimension must be divisible by 2 for ConvStem'\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.flatten = flatten\n",
    "\n",
    "        stem = []\n",
    "        input_dim, output_dim = in_chans, embed_dim // 8\n",
    "        for l in range(4):\n",
    "            stride = 2 if l < 3 else 1\n",
    "            stem.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=stride, padding=1, bias=False))\n",
    "            stem.append(nn.BatchNorm2d(output_dim))\n",
    "            stem.append(nn.ReLU(inplace=True))\n",
    "            input_dim = output_dim\n",
    "            output_dim *= 2\n",
    "        stem.append(nn.Conv2d(input_dim, embed_dim, kernel_size=1))\n",
    "        self.proj = nn.Sequential(*stem)\n",
    "\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "        else:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _BatchNorm1d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super(_BatchNorm1d, self).__init__()\n",
    "        self.norm = nn.BatchNorm1d(num_features=num_features,\n",
    "                                   eps=eps,\n",
    "                                   momentum=momentum,\n",
    "                                   affine=affine,\n",
    "                                   track_running_stats=track_running_stats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(x.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "\n",
    "class BlockFusion(nn.Module):\n",
    "    def __init__(self, dim, block4_identity=True, discrete=True):\n",
    "        super(BlockFusion, self).__init__()\n",
    "        self.proj_blocks = self._make_proj_block(dim * 4, dim)\n",
    "        self._discrete = discrete\n",
    "\n",
    "    def _make_proj_block(self, in_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            _BatchNorm1d(in_dim),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_dim, out_dim, bias=True)\n",
    "        )\n",
    "\n",
    "    def _unpool(self, block, label):\n",
    "        bs, ns, cs = block.shape\n",
    "        label = label.unsqueeze(2).expand(-1, -1, cs)\n",
    "        return torch.gather(block, 1, label)\n",
    "\n",
    "    def _proj_block_operations(self, x, cls_token, proj_block):\n",
    "        cls_x = torch.cat([cls_token, x], dim=1)\n",
    "        cls_x = proj_block(cls_x).type_as(x)\n",
    "        return cls_x[:, 1:, :], cls_x[:, :1, :]\n",
    "\n",
    "    def forward(self, block1, block2, block3, block4,\n",
    "                cls_token1, cls_token2, cls_token3, cls_token4,\n",
    "                logit1_2, logit2_3, logit3_4):\n",
    "\n",
    "        if not self._discrete:\n",
    "            raise NotImplementedError(\"Only support discrete unpooling\")\n",
    "\n",
    "        label1_2 = torch.argmax(logit1_2, dim=-1)\n",
    "        label2_3 = torch.argmax(logit2_3, dim=-1)\n",
    "        label3_4 = torch.argmax(logit3_4, dim=-1)\n",
    "\n",
    "        label1_3 = torch.gather(label2_3, 1, label1_2)\n",
    "        label1_4 = torch.gather(label3_4, 1, label1_3)\n",
    "\n",
    "        block2 = self._unpool(block2, label1_2)\n",
    "        block3 = self._unpool(block3, label1_3)\n",
    "        block4 = self._unpool(block4, label1_4)\n",
    "\n",
    "        out_block = torch.cat([block1, block2, block3, block4], dim=-1)\n",
    "        out_cls_token = torch.cat([cls_token1, cls_token2, cls_token3, cls_token4], dim=-1)\n",
    "\n",
    "        out_block, out_cls_token = self._proj_block_operations(\n",
    "            out_block, out_cls_token, self.proj_blocks)\n",
    "\n",
    "        return out_block, out_cls_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:51.987323Z",
     "iopub.status.busy": "2025-06-12T04:30:51.987134Z",
     "iopub.status.idle": "2025-06-12T04:30:52.005716Z",
     "shell.execute_reply": "2025-06-12T04:30:52.005050Z",
     "shell.execute_reply.started": "2025-06-12T04:30:51.987309Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cast_models/engine_hier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cast_models/engine_hier.py\n",
    "import math\n",
    "import sys\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm.utils import accuracy, ModelEma\n",
    "from cast_models.mixup_hier import Mixup\n",
    "from cast_models.losses import DistillationLoss\n",
    "from cast_models import utils\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    criterion: DistillationLoss,\n",
    "    data_loader: Iterable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    loss_scaler=None,\n",
    "    max_norm: float = 0,\n",
    "    model_ema: Optional[ModelEma] = None,\n",
    "    mixup_fn: Optional[Mixup] = None,\n",
    "    set_training_mode=True,\n",
    "    args=None,\n",
    "    task_classes: Optional[int] = None\n",
    "):\n",
    "    model.train(set_training_mode)\n",
    "    metric_logger = utils.MetricLogger(delimiter=\" \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{median:.6f} ({global_avg:.6f})'))\n",
    "    header = f'Epoch: [{epoch}]'\n",
    "    print_freq = 10\n",
    "\n",
    "    if args.globalkl:\n",
    "        gk_criterion = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    for batch_idx, batch in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        samples = batch[\"image\"].to(device, non_blocking=True)\n",
    "        segments = batch[\"segments\"].to(device, non_blocking=True)\n",
    "        targets = batch[\"variant\"].to(device, non_blocking=True)\n",
    "        family_targets = batch[\"family\"].to(device, non_blocking=True)\n",
    "        mf_targets = batch.get(\"maker\")\n",
    "        if mf_targets is not None:\n",
    "            mf_targets = mf_targets.to(device, non_blocking=True)\n",
    "\n",
    "        if mixup_fn is not None:\n",
    "            label_list = [targets, family_targets]\n",
    "            if mf_targets is not None:\n",
    "                label_list.append(mf_targets)\n",
    "            samples, *mixed_targets = mixup_fn(samples, label_list)\n",
    "            targets, family_targets = mixed_targets[:2]\n",
    "            mf_targets = mixed_targets[2] if len(mixed_targets) == 3 else None\n",
    "\n",
    "        model_output = model(samples, segments)\n",
    "        if isinstance(model_output, tuple):\n",
    "            if len(model_output) == 3:\n",
    "                outputs, family_out, manu_out = model_output\n",
    "            else:\n",
    "                outputs, family_out = model_output\n",
    "                manu_out = None\n",
    "        else:\n",
    "            raise ValueError(\"Model must return a tuple (variant, family [, maker])\")\n",
    "\n",
    "        if args.bce_loss:\n",
    "            def to_soft(targets_tensor, num_classes):\n",
    "                if targets_tensor.ndim == 2:\n",
    "                    return targets_tensor\n",
    "                return F.one_hot(targets_tensor, num_classes=num_classes).float()\n",
    "\n",
    "            targets = to_soft(targets, outputs.shape[1])\n",
    "            family_targets = to_soft(family_targets, family_out.shape[1])\n",
    "            if mf_targets is not None:\n",
    "                mf_targets = to_soft(mf_targets, manu_out.shape[1])\n",
    "        else:\n",
    "            if targets.ndim == 2:\n",
    "                targets = targets.argmax(dim=1)\n",
    "            if family_targets.ndim == 2:\n",
    "                family_targets = family_targets.argmax(dim=1)\n",
    "            if mf_targets is not None and mf_targets.ndim == 2:\n",
    "                mf_targets = mf_targets.argmax(dim=1)\n",
    "\n",
    "        #  Compute losses\n",
    "        loss_species = criterion(samples, outputs, targets)\n",
    "        loss_family = criterion(samples, family_out, family_targets)\n",
    "        loss = loss_species + loss_family\n",
    "\n",
    "        if mf_targets is not None:\n",
    "            loss_manufacturer = criterion(samples, manu_out, mf_targets)\n",
    "            loss += loss_manufacturer\n",
    "\n",
    "        if args.globalkl:\n",
    "            gk_outputs = [family_out, outputs]\n",
    "            gk_targets = [family_targets, targets]\n",
    "            if mf_targets is not None:\n",
    "                gk_outputs.insert(0, manu_out)\n",
    "                gk_targets.insert(0, mf_targets)\n",
    "\n",
    "            gk_outputs = torch.cat(gk_outputs, dim=1)\n",
    "            gk_outputs = F.log_softmax(gk_outputs, dim=1)\n",
    "\n",
    "            norm_targets = []\n",
    "            for out, tgt in zip([manu_out if mf_targets is not None else family_out, family_out, outputs][-len(gk_targets):], gk_targets):\n",
    "                if tgt.ndim == 1:\n",
    "                    tgt = F.one_hot(tgt, num_classes=out.shape[1]).float()\n",
    "                norm_targets.append(tgt)\n",
    "            gk_targets = F.normalize(torch.cat(norm_targets, dim=1), p=1, dim=1)\n",
    "\n",
    "            gk_loss = gk_criterion(gk_outputs, gk_targets)\n",
    "            loss += gk_loss * args.gk_weight\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if loss_scaler is not None:\n",
    "            loss_scaler(\n",
    "                loss, optimizer,\n",
    "                clip_grad=max_norm if max_norm > 0 else None,\n",
    "                parameters=model.parameters(),\n",
    "                create_graph=False\n",
    "            )\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if max_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        metric_logger.update(sploss=loss_species.item())\n",
    "        metric_logger.update(fmloss=loss_family.item())\n",
    "        if mf_targets is not None:\n",
    "            metric_logger.update(mfloss=loss_manufacturer.item())\n",
    "        if args.globalkl:\n",
    "            metric_logger.update(gk_loss=gk_loss.item())\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            def get_hard(t):\n",
    "                return t.argmax(dim=1) if t.ndim == 2 else t\n",
    "\n",
    "            acc1, acc5 = accuracy(outputs, get_hard(targets), topk=(1, 5))\n",
    "            fam_acc1 = accuracy(family_out, get_hard(family_targets), topk=(1,))[0]\n",
    "            metric_logger.meters['acc1'].update(acc1.item(), n=samples.size(0))\n",
    "            metric_logger.meters['acc5'].update(acc5.item(), n=samples.size(0))\n",
    "            metric_logger.meters['family_acc1'].update(fam_acc1.item(), n=samples.size(0))\n",
    "\n",
    "            if mf_targets is not None:\n",
    "                manu_acc1 = accuracy(manu_out, get_hard(mf_targets), topk=(1,))[0]\n",
    "                metric_logger.meters['manu_acc1'].update(manu_acc1.item(), n=samples.size(0))\n",
    "\n",
    "        if epoch == 0 and batch_idx == 0:\n",
    "            print(\"\\nðŸ” Gradient Check (First Batch Only):\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad and param.grad is None:\n",
    "                    print(f\"ðŸš© {name}: No gradient!\")\n",
    "                elif param.requires_grad:\n",
    "                    print(f\"âœ… {name}: grad norm = {param.grad.norm().item():.6f}\")\n",
    "\n",
    "    print(\" Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:52.006789Z",
     "iopub.status.busy": "2025-06-12T04:30:52.006567Z",
     "iopub.status.idle": "2025-06-12T04:30:52.024685Z",
     "shell.execute_reply": "2025-06-12T04:30:52.023998Z",
     "shell.execute_reply.started": "2025-06-12T04:30:52.006754Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cast_models/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cast_models/utils.py\n",
    "\"\"\"Utilility function for all.\n",
    "\"\"\"\n",
    "# This code is borrowed and re-implemented from:\n",
    "# https://github.com/jyhjinghwang/SegSort/blob/master/network/segsort/vis_utils.py\n",
    "# https://github.com/jyhjinghwang/SegSort/blob/master/network/segsort/common_utils.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "def resize_labels(labels, size):\n",
    "    \"\"\"Helper function to resize labels.\n",
    "\n",
    "    Args:\n",
    "        labels: A long tensor of shape `[batch_size, height, width]`.\n",
    "\n",
    "    Returns:\n",
    "        A long tensor of shape `[batch_size, new_height, new_width]`.\n",
    "    \"\"\"\n",
    "    n, h, w = labels.shape\n",
    "    labels = F.interpolate(labels.view(n, 1, h, w).float(),\n",
    "                           size=size,\n",
    "                           mode='nearest')\n",
    "    labels = labels.squeeze_(1).long()\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def calculate_principal_components(embeddings, num_components=3):\n",
    "    \"\"\"Calculates the principal components given the embedding features.\n",
    "\n",
    "    Args:\n",
    "      embeddings: A 2-D float tensor of shape `[num_pixels, embedding_dims]`.\n",
    "      num_components: An integer indicates the number of principal\n",
    "        components to return.\n",
    "\n",
    "    Returns:\n",
    "      A 2-D float tensor of shape `[num_pixels, num_components]`.\n",
    "    \"\"\"\n",
    "    embeddings = embeddings - torch.mean(embeddings, 0, keepdim=True)\n",
    "    _, _, v = torch.svd(embeddings)\n",
    "    return v[:, :num_components]\n",
    "\n",
    "\n",
    "def pca(embeddings, num_components=3, principal_components=None):\n",
    "    \"\"\"Conducts principal component analysis on the embedding features.\n",
    "\n",
    "    This function is used to reduce the dimensionality of the embedding.\n",
    "\n",
    "    Args:\n",
    "        embeddings: An N-D float tensor with shape with the \n",
    "            last dimension as `embedding_dim`.\n",
    "        num_components: The number of principal components.\n",
    "        principal_components: A 2-D float tensor used to convert the\n",
    "            embedding features to PCA'ed space, also known as the U matrix\n",
    "            from SVD. If not given, this function will calculate the\n",
    "            principal_components given inputs.\n",
    "\n",
    "    Returns:\n",
    "        A N-D float tensor with the last dimension as  `num_components`.\n",
    "    \"\"\"\n",
    "    shape = embeddings.shape\n",
    "    embeddings = embeddings.view(-1, shape[-1])\n",
    "\n",
    "    if principal_components is None:\n",
    "        principal_components = calculate_principal_components(\n",
    "            embeddings, num_components)\n",
    "    embeddings = torch.mm(embeddings, principal_components)\n",
    "\n",
    "    new_shape = list(shape[:-1]) + [num_components]\n",
    "    embeddings = embeddings.view(new_shape)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def one_hot(labels, max_label=None):\n",
    "    \"\"\"Transform long labels into one-hot format.\n",
    "\n",
    "    Args:\n",
    "        labels: An N-D long tensor.\n",
    "\n",
    "    Returns:\n",
    "        An (N+1)-D long tensor.\n",
    "    \"\"\"\n",
    "    if max_label is None:\n",
    "        max_label = labels.max() + 1\n",
    "\n",
    "    shape = labels.shape\n",
    "    labels = labels.view(-1, 1)\n",
    "    one_hot_labels = torch.zeros((labels.shape[0], max_label),\n",
    "                                 dtype=torch.long,\n",
    "                                 device=labels.device)\n",
    "    one_hot_labels = one_hot_labels.scatter_(1, labels, 1)\n",
    "\n",
    "    new_shape = list(shape) + [max_label]\n",
    "    one_hot_labels = one_hot_labels.view(new_shape)\n",
    "\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def normalize_embedding(embeddings, eps=1e-12):\n",
    "    \"\"\"Normalizes embedding by L2 norm.\n",
    "\n",
    "    This function is used to normalize embedding so that the\n",
    "    embedding features lie on a unit hypersphere.\n",
    "\n",
    "    Args:\n",
    "      embeddings: An N-D float tensor with feature embedding in\n",
    "        the last dimension.\n",
    "\n",
    "    Returns:\n",
    "      An N-D float tensor with the same shape as input embedding\n",
    "      with feature embedding normalized by L2 norm in the last\n",
    "      dimension.\n",
    "    \"\"\"\n",
    "    norm = torch.norm(embeddings, dim=-1, keepdim=True)\n",
    "    norm = torch.where(torch.ge(norm, eps),\n",
    "                       norm,\n",
    "                       torch.ones_like(norm).mul_(eps))\n",
    "    return embeddings / norm\n",
    "\n",
    "\n",
    "def segment_mean(x, index):\n",
    "    \"\"\"\n",
    "    Compute the mean of elements in `x` grouped by `index`.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape [N, D] or any shape where last dimension is feature dim.\n",
    "        index: LongTensor of shape [N], must be >= 0 and contiguous group indices.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [num_segments, D] where each row is mean of the segment.\n",
    "    \"\"\"\n",
    "    # Ensure x is 2D\n",
    "    x = x.reshape(-1, x.shape[-1])  # Use reshape instead of view for safety\n",
    "    index = index.reshape(-1)\n",
    "\n",
    "    # Check for negative indices which would cause runtime error\n",
    "    if torch.any(index < 0):\n",
    "        raise ValueError(\"segment_mean error: `index` contains negative values.\")\n",
    "\n",
    "    max_index = index.max().item() + 1  # Get total number of segments\n",
    "\n",
    "    sum_x = torch.zeros((max_index, x.shape[-1]), dtype=x.dtype, device=x.device)\n",
    "    count = torch.zeros((max_index,), dtype=x.dtype, device=x.device)\n",
    "\n",
    "    count = count.scatter_add(0, index, torch.ones_like(index, dtype=x.dtype))\n",
    "    index_2d = index.unsqueeze(1).expand(-1, x.shape[-1])\n",
    "    sum_x = sum_x.scatter_add(0, index_2d, x)\n",
    "\n",
    "    # Avoid divide-by-zero (if any count is 0)\n",
    "    count = torch.clamp(count, min=1.0)\n",
    "    mean_x = sum_x / count.unsqueeze(1)\n",
    "\n",
    "    return mean_x\n",
    "\n",
    "\n",
    "def segment_mean_nd(x, index, start_dim=-3, end_dim=-2):\n",
    "    \"\"\"\n",
    "    Batched segment-wise mean pooling for N-D tensors using labels.\n",
    "\n",
    "    Args:\n",
    "        x: Float tensor of shape [B, ..., H, W, C] where C is feature dim.\n",
    "        index: Long tensor of shape [B, ..., H, W] containing segment labels.\n",
    "        start_dim: Start of spatial dims (default: -3 for [B, H, W, C]).\n",
    "        end_dim: End of spatial dims (default: -2).\n",
    "\n",
    "    Returns:\n",
    "        mean_x: Float tensor of shape [B, K, C] where K = number of segments.\n",
    "    \"\"\"\n",
    "    # Normalize dims\n",
    "    if start_dim < 0:\n",
    "        start_dim = x.dim() + start_dim\n",
    "    if end_dim < 0:\n",
    "        end_dim = x.dim() + end_dim\n",
    "\n",
    "    batch_dims = list(x.shape[:start_dim])\n",
    "    C = x.shape[-1]\n",
    "\n",
    "    # Flatten batch + spatial dims\n",
    "    x = x.reshape(-1, C)\n",
    "    index = index.reshape(-1)\n",
    "\n",
    "    if torch.any(index < 0):\n",
    "        raise ValueError(\"segment_mean_nd error: `index` contains negative values.\")\n",
    "\n",
    "    B = batch_dims[0]\n",
    "    max_seg_id = index.max().item() + 1\n",
    "\n",
    "    # Encode batch dimension into segment index to keep segments separate per sample\n",
    "    batch_ids = torch.arange(B, device=index.device).repeat_interleave(index.numel() // B)\n",
    "    global_index = batch_ids * max_seg_id + index\n",
    "\n",
    "    # Use the safe segment_mean implementation\n",
    "    mean_x = segment_mean(x, global_index)\n",
    "\n",
    "    # Fix output shape: [B, max_seg_id, C]\n",
    "    expected_len = B * max_seg_id\n",
    "    if mean_x.shape[0] < expected_len:\n",
    "        mean_x = F.pad(mean_x, (0, 0, 0, expected_len - mean_x.shape[0]))\n",
    "\n",
    "    mean_x = mean_x.reshape(B, max_seg_id, C)\n",
    "    return mean_x\n",
    "\n",
    "\n",
    "def farthest_point_sampling(x, k):\n",
    "    \"\"\"\n",
    "    x: (B, N, C) tensor\n",
    "    k: number of samples\n",
    "    Return: (B, k) indices\n",
    "    \"\"\"\n",
    "    B, N, C = x.shape\n",
    "    indices = torch.zeros((B, k), dtype=torch.long, device=x.device)\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long, device=x.device)\n",
    "    distances = torch.full((B, N), float('inf'), device=x.device)\n",
    "\n",
    "    for i in range(k):\n",
    "        indices[:, i] = farthest\n",
    "        centroid = x[torch.arange(B), farthest].unsqueeze(1)  # (B, 1, C)\n",
    "        dist = ((x - centroid) ** 2).sum(-1)  # (B, N)\n",
    "        mask = dist < distances\n",
    "        distances[mask] = dist[mask]\n",
    "        farthest = distances.max(-1)[1]\n",
    "\n",
    "    return indices\n",
    "\n",
    "class NativeScalerWithGradNormCount:\n",
    "    def __init__(self):\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False):\n",
    "        self.scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if clip_grad is not None:\n",
    "            self.scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "        self.scaler.step(optimizer)\n",
    "        self.scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "class SmoothedValue:\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a window.\"\"\"\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt if fmt else \"{median:.4f} ({global_avg:.4f})\"\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count if self.count > 0 else 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(median=self.median, global_avg=self.global_avg)\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.delimiter.join(f\"{name}: {meter}\" for name, meter in self.meters.items())\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        start_time = time.time()\n",
    "        for obj in iterable:\n",
    "            yield obj\n",
    "            if i % print_freq == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print_str = [header or \"\", f\"[{i}/{len(iterable)}]\"]\n",
    "                print_str += [f\"{name}: {str(meter)}\" for name, meter in self.meters.items()]\n",
    "                print_str.append(f\"time: {elapsed:.2f}s\")\n",
    "                print(self.delimiter.join(print_str))\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:52.025844Z",
     "iopub.status.busy": "2025-06-12T04:30:52.025561Z",
     "iopub.status.idle": "2025-06-12T04:30:52.044335Z",
     "shell.execute_reply": "2025-06-12T04:30:52.043519Z",
     "shell.execute_reply.started": "2025-06-12T04:30:52.025816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cast_models/mixup_hier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cast_models/mixup_hier.py\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def one_hot(x, num_classes, on_value=1., off_value=0.):\n",
    "    x = x.long().view(-1, 1)\n",
    "    if torch.any(x < 0) or torch.any(x >= num_classes):\n",
    "        print(f\" ERROR: Found out-of-bound label in one_hot! Min: {x.min().item()}, Max: {x.max().item()}, Num classes: {num_classes}\")\n",
    "        raise ValueError(\"Labels out of range for one_hot encoding.\")\n",
    "    return torch.full((x.size()[0], num_classes), off_value, device=x.device).scatter_(1, x, on_value)\n",
    "\n",
    "def mixup_target(target, num_classes, lam=1., smoothing=0.0):\n",
    "    off_value = smoothing / num_classes\n",
    "    on_value = 1. - smoothing + off_value\n",
    "    y1 = one_hot(target, num_classes, on_value=on_value, off_value=off_value)\n",
    "    y2 = one_hot(target.flip(0), num_classes, on_value=on_value, off_value=off_value)\n",
    "    return y1 * lam + y2 * (1. - lam)\n",
    "\n",
    "def rand_bbox(img_shape, lam, margin=0., count=None):\n",
    "    ratio = np.sqrt(1 - lam)\n",
    "    img_h, img_w = img_shape[-2:]\n",
    "    cut_h, cut_w = int(img_h * ratio), int(img_w * ratio)\n",
    "    margin_y, margin_x = int(margin * cut_h), int(margin * cut_w)\n",
    "    cy = np.random.randint(0 + margin_y, img_h - margin_y, size=count)\n",
    "    cx = np.random.randint(0 + margin_x, img_w - margin_x, size=count)\n",
    "    yl = np.clip(cy - cut_h // 2, 0, img_h)\n",
    "    yh = np.clip(cy + cut_h // 2, 0, img_h)\n",
    "    xl = np.clip(cx - cut_w // 2, 0, img_w)\n",
    "    xh = np.clip(cx + cut_w // 2, 0, img_w)\n",
    "    return yl, yh, xl, xh\n",
    "\n",
    "def rand_bbox_minmax(img_shape, minmax, count=None):\n",
    "    assert len(minmax) == 2\n",
    "    img_h, img_w = img_shape[-2:]\n",
    "    cut_h = np.random.randint(int(img_h * minmax[0]), int(img_h * minmax[1]), size=count)\n",
    "    cut_w = np.random.randint(int(img_w * minmax[0]), int(img_w * minmax[1]), size=count)\n",
    "    yl = np.random.randint(0, img_h - cut_h, size=count)\n",
    "    xl = np.random.randint(0, img_w - cut_w, size=count)\n",
    "    yu = yl + cut_h\n",
    "    xu = xl + cut_w\n",
    "    return yl, yu, xl, xu\n",
    "\n",
    "def cutmix_bbox_and_lam(img_shape, lam, ratio_minmax=None, correct_lam=True, count=None):\n",
    "    if ratio_minmax is not None:\n",
    "        yl, yu, xl, xu = rand_bbox_minmax(img_shape, ratio_minmax, count=count)\n",
    "    else:\n",
    "        yl, yu, xl, xu = rand_bbox(img_shape, lam, count=count)\n",
    "    if correct_lam or ratio_minmax is not None:\n",
    "        bbox_area = (yu - yl) * (xu - xl)\n",
    "        lam = 1. - bbox_area / float(img_shape[-2] * img_shape[-1])\n",
    "    return (yl, yu, xl, xu), lam\n",
    "\n",
    "class Mixup:\n",
    "    def __init__(self, mixup_alpha=1., cutmix_alpha=0., cutmix_minmax=None, prob=1.0, switch_prob=0.5,\n",
    "                 mode='batch', correct_lam=True, label_smoothing=0.1, num_classes=[100, 70, 30]):\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.cutmix_alpha = cutmix_alpha\n",
    "        self.cutmix_minmax = cutmix_minmax\n",
    "        if self.cutmix_minmax is not None:\n",
    "            assert len(self.cutmix_minmax) == 2\n",
    "            self.cutmix_alpha = 1.0\n",
    "        self.mix_prob = prob\n",
    "        self.switch_prob = switch_prob\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.num_classes = num_classes\n",
    "        self.mode = mode\n",
    "        self.correct_lam = correct_lam\n",
    "        self.mixup_enabled = True\n",
    "\n",
    "    def _params_per_elem(self, batch_size):\n",
    "        lam = np.ones(batch_size, dtype=np.float32)\n",
    "        use_cutmix = np.zeros(batch_size, dtype=bool)\n",
    "        if self.mixup_enabled:\n",
    "            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:\n",
    "                use_cutmix = np.random.rand(batch_size) < self.switch_prob\n",
    "                lam_mix = np.where(\n",
    "                    use_cutmix,\n",
    "                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size),\n",
    "                    np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size))\n",
    "            elif self.mixup_alpha > 0.:\n",
    "                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size)\n",
    "            elif self.cutmix_alpha > 0.:\n",
    "                use_cutmix = np.ones(batch_size, dtype=bool)\n",
    "                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size)\n",
    "            else:\n",
    "                raise ValueError(\"Either mixup_alpha or cutmix_alpha must be > 0\")\n",
    "            lam = np.where(np.random.rand(batch_size) < self.mix_prob, lam_mix.astype(np.float32), lam)\n",
    "        return lam, use_cutmix\n",
    "\n",
    "    def _params_per_batch(self):\n",
    "        lam = 1.\n",
    "        use_cutmix = False\n",
    "        if self.mixup_enabled and np.random.rand() < self.mix_prob:\n",
    "            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:\n",
    "                use_cutmix = np.random.rand() < self.switch_prob\n",
    "                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha) if use_cutmix else \\\n",
    "                    np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "            elif self.mixup_alpha > 0.:\n",
    "                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "            elif self.cutmix_alpha > 0.:\n",
    "                use_cutmix = True\n",
    "                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n",
    "            else:\n",
    "                raise ValueError(\"Either mixup_alpha or cutmix_alpha must be > 0\")\n",
    "            lam = float(lam_mix)\n",
    "        return lam, use_cutmix\n",
    "\n",
    "    def _mix_batch(self, x):\n",
    "        lam, use_cutmix = self._params_per_batch()\n",
    "        if lam == 1.:\n",
    "            return 1.\n",
    "        if use_cutmix:\n",
    "            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n",
    "                x.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n",
    "            x[:, :, yl:yh, xl:xh] = x.flip(0)[:, :, yl:yh, xl:xh]\n",
    "        else:\n",
    "            x_flipped = x.flip(0).mul_(1. - lam)\n",
    "            x.mul_(lam).add_(x_flipped)\n",
    "        return lam\n",
    "\n",
    "    def __call__(self, x, target_list):\n",
    "        assert len(x) % 2 == 0, 'Batch size should be even for mixup'\n",
    "        lam = self._mix_batch(x) if self.mode == 'batch' else 1.0  # You can expand support for 'elem' or 'pair' if needed\n",
    "        targets_encoded = []\n",
    "\n",
    "        for i, targets in enumerate(target_list):\n",
    "            targets_encoded.append(mixup_target(targets, self.num_classes[i], lam, self.label_smoothing))\n",
    "\n",
    "        return (x, *targets_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:52.045946Z",
     "iopub.status.busy": "2025-06-12T04:30:52.045251Z",
     "iopub.status.idle": "2025-06-12T04:30:52.062473Z",
     "shell.execute_reply": "2025-06-12T04:30:52.061650Z",
     "shell.execute_reply.started": "2025-06-12T04:30:52.045924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cast_models/losses.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cast_models/losses.py\n",
    "# Copyright (c) 2015-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "\"\"\"\n",
    "Implements the knowledge distillation loss\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class DistillationLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This module wraps a standard criterion and adds an extra knowledge distillation loss by\n",
    "    taking a teacher model prediction and using it as additional supervision.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_criterion: torch.nn.Module, teacher_model: torch.nn.Module,\n",
    "                 distillation_type: str, alpha: float, tau: float):\n",
    "        super().__init__()\n",
    "        self.base_criterion = base_criterion\n",
    "        self.teacher_model = teacher_model\n",
    "        assert distillation_type in ['none', 'soft', 'hard']\n",
    "        self.distillation_type = distillation_type\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, inputs, outputs, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: The original inputs that are fed to the teacher model\n",
    "            outputs: the outputs of the model to be trained. It is expected to be\n",
    "                either a Tensor, or a Tuple[Tensor, Tensor], with the original output\n",
    "                in the first position and the distillation predictions as the second output\n",
    "            labels: the labels for the base criterion (can be hard or soft targets)\n",
    "        \"\"\"\n",
    "        outputs_kd = None\n",
    "        if not isinstance(outputs, torch.Tensor):\n",
    "            # assume that the model outputs a tuple of [outputs, outputs_kd]\n",
    "            outputs, outputs_kd = outputs\n",
    "\n",
    "        #  Handle BCE vs CE correctly with soft labels\n",
    "        if labels.ndim == 2:\n",
    "            if isinstance(self.base_criterion, torch.nn.CrossEntropyLoss):\n",
    "                raise ValueError(\"CrossEntropyLoss does not support soft targets. Use BCEWithLogitsLoss instead.\")\n",
    "            base_loss = self.base_criterion(outputs, labels)\n",
    "        else:\n",
    "            base_loss = self.base_criterion(outputs, labels)\n",
    "\n",
    "        if self.distillation_type == 'none':\n",
    "            return base_loss\n",
    "\n",
    "        if outputs_kd is None:\n",
    "            raise ValueError(\"When knowledge distillation is enabled, the model is \"\n",
    "                             \"expected to return a Tuple[Tensor, Tensor] with the output of the \"\n",
    "                             \"class_token and the dist_token\")\n",
    "\n",
    "        # don't backprop through the teacher\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(inputs)\n",
    "\n",
    "        if self.distillation_type == 'soft':\n",
    "            T = self.tau\n",
    "            distillation_loss = F.kl_div(\n",
    "                F.log_softmax(outputs_kd / T, dim=1),\n",
    "                F.log_softmax(teacher_outputs / T, dim=1),\n",
    "                reduction='sum',\n",
    "                log_target=True\n",
    "            ) * (T * T) / outputs_kd.numel()\n",
    "\n",
    "        elif self.distillation_type == 'hard':\n",
    "            distillation_loss = F.cross_entropy(outputs_kd, teacher_outputs.argmax(dim=1))\n",
    "\n",
    "        loss = base_loss * (1 - self.alpha) + distillation_loss * self.alpha\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:52.063401Z",
     "iopub.status.busy": "2025-06-12T04:30:52.063177Z",
     "iopub.status.idle": "2025-06-12T04:30:52.144587Z",
     "shell.execute_reply": "2025-06-12T04:30:52.143933Z",
     "shell.execute_reply.started": "2025-06-12T04:30:52.063376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#  Base path\n",
    "base_path = \"/kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data\"\n",
    "\n",
    "#  Load mapping files\n",
    "def load_mapping(file_path, label_type):\n",
    "    mapping = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            if len(parts) == 2:\n",
    "                mapping.append((parts[0], parts[1]))\n",
    "    return pd.DataFrame(mapping, columns=[\"image\", label_type])\n",
    "\n",
    "variant_df = load_mapping(f\"{base_path}/images_variant_trainval.txt\", \"variant\")\n",
    "family_df = load_mapping(f\"{base_path}/images_family_trainval.txt\", \"family\")\n",
    "maker_df = load_mapping(f\"{base_path}/images_manufacturer_trainval.txt\", \"maker\")\n",
    "\n",
    "#  Merge labels on 'image'\n",
    "df = variant_df.merge(family_df, on=\"image\").merge(maker_df, on=\"image\")\n",
    "\n",
    "#  Group by variant to get canonical family & maker\n",
    "hierarchy = df.groupby(\"variant\")[[\"family\", \"maker\"]].agg(lambda x: x.mode().iloc[0]).reset_index()\n",
    "\n",
    "#  Create ID maps\n",
    "variant_to_id = {v: i for i, v in enumerate(hierarchy[\"variant\"].sort_values().unique())}\n",
    "family_to_id = {f: i for i, f in enumerate(hierarchy[\"family\"].sort_values().unique())}\n",
    "maker_to_id = {m: i for i, m in enumerate(hierarchy[\"maker\"].sort_values().unique())}\n",
    "\n",
    "#  Map IDs\n",
    "hierarchy[\"variant_id\"] = hierarchy[\"variant\"].map(variant_to_id)\n",
    "hierarchy[\"family_id\"] = hierarchy[\"family\"].map(family_to_id)\n",
    "hierarchy[\"maker_id\"] = hierarchy[\"maker\"].map(maker_to_id)\n",
    "\n",
    "#  Optional: Create lookup dicts for later use\n",
    "variant_to_family = hierarchy.set_index(\"variant_id\")[\"family_id\"].to_dict()\n",
    "variant_to_maker = hierarchy.set_index(\"variant_id\")[\"maker_id\"].to_dict()\n",
    "\n",
    "output_path = \"/kaggle/working/hierarchy_full.csv\"\n",
    "hierarchy.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "print(\"âœ… hierarchy_full.csv saved with columns:\", hierarchy.columns.tolist())\n",
    "print(f\"ðŸ”¢ Classes: {len(variant_to_id)} variants, {len(family_to_id)} families, {len(maker_to_id)} makers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T04:30:52.146407Z",
     "iopub.status.busy": "2025-06-12T04:30:52.145531Z",
     "iopub.status.idle": "2025-06-12T06:14:08.932394Z",
     "shell.execute_reply": "2025-06-12T06:14:08.931534Z",
     "shell.execute_reply.started": "2025-06-12T04:30:52.146386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cast_models.aircraft_dataset import AircraftHCASTDataset\n",
    "from cast_models.mixup_hier import Mixup\n",
    "from cast_models.cast import cast_small\n",
    "from cast_models.utils import NativeScalerWithGradNormCount\n",
    "\n",
    "# Configuration\n",
    "class Args:\n",
    "    lr = 3e-4\n",
    "    epochs = 200\n",
    "    weight_decay = 0.05\n",
    "    num_workers = 4\n",
    "    mixup = 0.2\n",
    "    cutmix = 0.2\n",
    "    bce_loss = False\n",
    "    globalkl = True\n",
    "    gk_weight = 1.5\n",
    "    warmup_epochs = 20\n",
    "    min_lr = 1e-6\n",
    "    early_stopping_patience = 15\n",
    "\n",
    "args = Args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Utils\n",
    "class SoftTargetCrossEntropy(nn.Module):\n",
    "    def forward(self, logits, soft_targets):\n",
    "        return torch.sum(-soft_targets * F.log_softmax(logits, dim=-1), dim=-1).mean()\n",
    "\n",
    "class RemapLabels(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, variant_map, family_map, maker_map):\n",
    "        self.subset = subset\n",
    "        self.variant_map = variant_map\n",
    "        self.family_map = family_map\n",
    "        self.maker_map = maker_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.subset[idx]\n",
    "        item[\"variant\"] = torch.tensor(self.variant_map[int(item[\"variant\"])], dtype=torch.long)\n",
    "        item[\"family\"]  = torch.tensor(self.family_map[int(item[\"family\"])],   dtype=torch.long)\n",
    "        item[\"maker\"]   = torch.tensor(self.maker_map[int(item[\"maker\"])],     dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Load task split\n",
    "with open(\"task_split.json\", \"r\") as f:\n",
    "    task_split = json.load(f)\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandAugment(num_ops=9, magnitude=5),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "full_dataset = AircraftHCASTDataset(\n",
    "    image_dir=\"/kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images\",\n",
    "    label_file=\"hierarchy_full.csv\",\n",
    "    image_split_file=\"/kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images_variant_trainval.txt\",\n",
    "    mapping_file=\"mapping.txt\",\n",
    "    transform=transform\n",
    ")\n",
    "print(f\"Loaded dataset: {len(full_dataset)} samples\")\n",
    "\n",
    "# Label remapping\n",
    "global_var_map = {orig: i for i, orig in enumerate(sorted(set(full_dataset.variants)))}\n",
    "global_fam_map = {orig: i for i, orig in enumerate(sorted(set(full_dataset.families)))}\n",
    "global_man_map = {orig: i for i, orig in enumerate(sorted(set(full_dataset.makers)))}\n",
    "nb_classes = [len(global_var_map), len(global_fam_map), len(global_man_map)]\n",
    "print(f\"Total classes: Variant={nb_classes[0]}, Family={nb_classes[1]}, Maker={nb_classes[2]}\")\n",
    "\n",
    "# Model setup\n",
    "model = cast_small(nb_classes=nb_classes).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "loss_scaler = NativeScalerWithGradNormCount()\n",
    "\n",
    "acc1_list, fam_acc_list, manu_acc_list = [], [], []\n",
    "cumulative_eval_indices = []\n",
    "\n",
    "for task_idx in range(1, 11):\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=args.epochs - args.warmup_epochs, eta_min=args.min_lr\n",
    "    )\n",
    "\n",
    "    task_name = f\"task_{task_idx}\"\n",
    "    variants_in_task = {v[\"variant\"] for v in task_split[task_name]}\n",
    "    orig_variant_ids = {\n",
    "        full_dataset.variants[i]\n",
    "        for i, name in enumerate(full_dataset.variant_names)\n",
    "        if name in variants_in_task\n",
    "    }\n",
    "    current_task_indices = [i for i, vid in enumerate(full_dataset.variants) if vid in orig_variant_ids]\n",
    "    cumulative_eval_indices.extend(current_task_indices)\n",
    "\n",
    "    train_subset = Subset(full_dataset, current_task_indices)\n",
    "    train_remapped = RemapLabels(train_subset, global_var_map, global_fam_map, global_man_map)\n",
    "    train_loader = DataLoader(train_remapped, batch_size=128, shuffle=True,\n",
    "                              num_workers=args.num_workers, drop_last=True, pin_memory=True)\n",
    "\n",
    "    mixup_fn = Mixup(mixup_alpha=args.mixup, cutmix_alpha=args.cutmix,\n",
    "                     num_classes=nb_classes, label_smoothing=0.1, mode=\"batch\")\n",
    "    loss_var = SoftTargetCrossEntropy()\n",
    "    loss_fam = SoftTargetCrossEntropy()\n",
    "    loss_man = SoftTargetCrossEntropy()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(f\"\\n--- Training on Task {task_idx} ({len(train_subset)} samples) ---\")\n",
    "    model.train()\n",
    "    for epoch in range(args.epochs):\n",
    "        if epoch < args.warmup_epochs:\n",
    "            new_lr = args.lr * (epoch + 1) / args.warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "        else:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            imgs = batch[\"image\"].to(device)\n",
    "            segs = batch[\"segments\"].to(device)\n",
    "            tgt_v = batch[\"variant\"].to(device)\n",
    "            tgt_f = batch[\"family\"].to(device)\n",
    "            tgt_m = batch[\"maker\"].to(device)\n",
    "\n",
    "            imgs, mt_v, mt_f, mt_m = mixup_fn(imgs, [tgt_v, tgt_f, tgt_m])\n",
    "            out_v, out_f, out_m = model(imgs, segs)\n",
    "\n",
    "            l_v = loss_var(out_v, mt_v)\n",
    "            l_f = loss_fam(out_f, mt_f)\n",
    "            l_m = loss_man(out_m, mt_m)\n",
    "            loss = l_v + l_f + args.gk_weight * l_m\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_scaler(loss, optimizer, parameters=model.parameters())\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Early stopping logic (on training loss â€” you can switch to validation accuracy if needed)\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        if avg_epoch_loss < best_val_acc or epoch == 0:\n",
    "            best_val_acc = avg_epoch_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= args.early_stopping_patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    # Evaluation on cumulative seen tasks\n",
    "    model.eval()\n",
    "    correct_v = correct_f = correct_m = total = 0\n",
    "    eval_subset = Subset(full_dataset, cumulative_eval_indices)\n",
    "    eval_remapped = RemapLabels(eval_subset, global_var_map, global_fam_map, global_man_map)\n",
    "    eval_loader = DataLoader(eval_remapped, batch_size=128, shuffle=False,\n",
    "                             num_workers=args.num_workers, drop_last=False, pin_memory=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            imgs = batch[\"image\"].to(device)\n",
    "            segs = batch[\"segments\"].to(device)\n",
    "            tgt_v = batch[\"variant\"].to(device)\n",
    "            tgt_f = batch[\"family\"].to(device)\n",
    "            tgt_m = batch[\"maker\"].to(device)\n",
    "\n",
    "            out_v, out_f, out_m = model(imgs, segs)\n",
    "            pred_v = out_v.argmax(1)\n",
    "            pred_f = out_f.argmax(1)\n",
    "            pred_m = out_m.argmax(1)\n",
    "\n",
    "            total += imgs.size(0)\n",
    "            correct_v += (pred_v == tgt_v).sum().item()\n",
    "            correct_f += (pred_f == tgt_f).sum().item()\n",
    "            correct_m += (pred_m == tgt_m).sum().item()\n",
    "\n",
    "    acc1_list.append(correct_v / total * 100)\n",
    "    fam_acc_list.append(correct_f / total * 100)\n",
    "    manu_acc_list.append(correct_m / total * 100)\n",
    "    \n",
    "    model_path = f\"hcast_task{task_idx}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\" Model for {task_name} saved as {model_path}\")\n",
    "\n",
    "    print(f\"CUMULATIVE ACCURACY after Task {task_idx}: \"\n",
    "          f\"Variant@1={acc1_list[-1]:.2f}% | Family@1={fam_acc_list[-1]:.2f}% | Maker@1={manu_acc_list[-1]:.2f}%\")\n",
    "\n",
    "# Plot accuracy\n",
    "tasks = np.arange(1, len(acc1_list)+1)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(tasks, acc1_list, marker='o', label='Variant Acc@1 (Cumulative)')\n",
    "plt.plot(tasks, fam_acc_list, marker='^', label='Family Acc@1 (Cumulative)')\n",
    "plt.plot(tasks, manu_acc_list, marker='x', label='Maker Acc@1 (Cumulative)')\n",
    "plt.xlabel(\"Task Number\")\n",
    "plt.ylabel(\"Cumulative Accuracy on All Seen Data (%)\")\n",
    "plt.title(\"Cumulative Accuracy Over Tasks\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cumulative_accuracy_updated.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 782089,
     "sourceId": 1364214,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
